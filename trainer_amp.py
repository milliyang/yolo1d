#!/usr/bin/env python3
"""
ÊîØÊåÅÊ∑∑ÂêàÁ≤æÂ∫¶ËÆ≠ÁªÉÁöÑYOLO1DËÆ≠ÁªÉÂô®
ÊèêÂçáËÆ≠ÁªÉÊïàÁéáÔºåÂáèÂ∞ëÂÜÖÂ≠ò‰ΩøÁî®
"""

import torch
import torch.nn as nn
from torch.amp import autocast, GradScaler
from typing import Dict, Any, Optional, Tuple
import logging

from trainer_base import BaseTrainer, YOLO1DTrainer
from utils import postprocess_for_metrics


class AMPTrainer(BaseTrainer):
    """Ëá™Âä®Ê∑∑ÂêàÁ≤æÂ∫¶ËÆ≠ÁªÉÂô®"""
    
    def __init__(self, 
                 model: nn.Module,
                 train_loader: torch.utils.data.DataLoader,
                 val_loader: torch.utils.data.DataLoader,
                 config: Dict[str, Any],
                 device: torch.device,
                 resume_from: Optional[str] = None):
        """
        ÂàùÂßãÂåñAMPËÆ≠ÁªÉÂô®
        
        Args:
            model: YOLO1DÊ®°Âûã
            train_loader: ËÆ≠ÁªÉÊï∞ÊçÆÂä†ËΩΩÂô®
            val_loader: È™åËØÅÊï∞ÊçÆÂä†ËΩΩÂô®
            config: ËÆ≠ÁªÉÈÖçÁΩÆ
            device: ËÆ°ÁÆóËÆæÂ§á
            resume_from: ÊÅ¢Â§çËÆ≠ÁªÉÁöÑÊ£ÄÊü•ÁÇπË∑ØÂæÑ
        """
        super().__init__(model, train_loader, val_loader, config, device, resume_from)
        
        # ËÆæÁΩÆÊ∑∑ÂêàÁ≤æÂ∫¶ËÆ≠ÁªÉ
        self.setup_amp()
    
    def setup_amp(self):
        """ËÆæÁΩÆËá™Âä®Ê∑∑ÂêàÁ≤æÂ∫¶ËÆ≠ÁªÉ"""
        if self.device.type == 'cuda':
            self.scaler = GradScaler('cuda')
            self.use_amp = True
            self.logger.info("‚úÖ ÂêØÁî®Ëá™Âä®Ê∑∑ÂêàÁ≤æÂ∫¶ËÆ≠ÁªÉ")
        else:
            self.use_amp = False
            self.logger.info("‚ö†Ô∏è CPUËÆæÂ§áÔºåÁ¶ÅÁî®Ê∑∑ÂêàÁ≤æÂ∫¶ËÆ≠ÁªÉ")
    
    def train_epoch(self, epoch: int) -> float:
        """ËÆ≠ÁªÉ‰∏Ä‰∏™epochÔºàÊ∑∑ÂêàÁ≤æÂ∫¶ÁâàÊú¨Ôºâ"""
        self.model.train()
        epoch_loss = 0.0
        
        pbar = self._create_progress_bar(epoch, "Training")
        
        for batch_idx, (signals, targets) in enumerate(pbar):
            try:
                signals = signals.to(self.device)
                targets = targets.to(self.device)
                
                # ÂâçÂêë‰º†Êí≠ÔºàÊ∑∑ÂêàÁ≤æÂ∫¶Ôºâ
                self.optimizer.zero_grad()
                
                if self.use_amp:
                    with autocast(device_type='cuda', dtype=torch.float16):
                        preds = self.model(signals)
                        loss, loss_items = self.criterion(preds, targets, self.model)
                else:
                    preds = self.model(signals)
                    loss, loss_items = self.criterion(preds, targets, self.model)
                
                # Ê£ÄÊü•ÊçüÂ§±ÊòØÂê¶ÊúâÊïà
                if torch.isnan(loss) or torch.isinf(loss):
                    self.logger.warning(f"Warning: Invalid loss at batch {batch_idx}: {loss}")
                    continue
                
                # ÂèçÂêë‰º†Êí≠ÔºàÊ∑∑ÂêàÁ≤æÂ∫¶Ôºâ
                if self.use_amp:
                    self.scaler.scale(loss).backward()
                    
                    # Ê¢ØÂ∫¶Ë£ÅÂâ™
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(), 
                        max_norm=self.config.get('grad_clip', 1.0)
                    )
                    
                    # ‰ºòÂåñÂô®Ê≠•È™§
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    loss.backward()
                    
                    # Ê¢ØÂ∫¶Ë£ÅÂâ™
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(), 
                        max_norm=self.config.get('grad_clip', 1.0)
                    )
                    
                    self.optimizer.step()
                
                # Êõ¥Êñ∞Â≠¶‰π†ÁéáÔºàOneCycleLRÔºâ
                if isinstance(self.scheduler, torch.optim.lr_scheduler.OneCycleLR):
                    self.scheduler.step()
                
                # ËÆ∞ÂΩïÊçüÂ§±
                epoch_loss += loss.item()
                
                # Êõ¥Êñ∞ËøõÂ∫¶Êù°
                self._update_progress_bar(pbar, loss, loss_items, batch_idx, epoch)
                
                # ËÆ∞ÂΩïÂà∞TensorBoard
                self._log_training_step(loss, loss_items, batch_idx, epoch)
                
            except Exception as e:
                self.logger.error(f"ËÆ≠ÁªÉÊ≠•È™§Âá∫Èîô (batch {batch_idx}): {e}")
                continue
        
        return epoch_loss / len(self.train_loader)
    
    def validate(self, epoch: int) -> Tuple[float, float]:
        """È™åËØÅÔºàÊ∑∑ÂêàÁ≤æÂ∫¶ÁâàÊú¨Ôºâ"""
        self.model.eval()
        val_loss = 0.0
        
        # ÈáçÁΩÆmAPÊåáÊ†á
        self.map_metric.reset()
        
        with torch.no_grad():
            pbar = self._create_progress_bar(epoch, "Validation")
            
            for batch_idx, (signals, targets) in enumerate(pbar):
                try:
                    signals = signals.to(self.device)
                    targets = targets.to(self.device)
                    
                    # ÂâçÂêë‰º†Êí≠ÔºàÊ∑∑ÂêàÁ≤æÂ∫¶Ôºâ
                    if self.use_amp:
                        with autocast(device_type='cuda', dtype=torch.float16):
                            preds = self.model(signals)
                            loss, loss_items = self.criterion(preds, targets, self.model)
                    else:
                        preds = self.model(signals)
                        loss, loss_items = self.criterion(preds, targets, self.model)
                    
                    val_loss += loss.item()
                    
                    # ËÆ°ÁÆómAP
                    self._update_map_metric(preds, targets, signals)
                    
                    # Êõ¥Êñ∞ËøõÂ∫¶Êù°
                    pbar.set_postfix({
                        'val_loss': f'{loss.item():.4f}',
                        'box': f'{loss_items[0]:.4f}',
                        'cls': f'{loss_items[1]:.4f}',
                        'dfl': f'{loss_items[2]:.4f}',
                    })
                    
                except Exception as e:
                    self.logger.error(f"È™åËØÅÊ≠•È™§Âá∫Èîô (batch {batch_idx}): {e}")
                    continue
        
        # ËÆ°ÁÆóÂπ≥ÂùáÊçüÂ§±ÂíåmAP
        avg_val_loss = val_loss / len(self.val_loader)
        
        # Ê£ÄÊü•mAPÊåáÊ†áÊòØÂê¶ÊúâÊï∞ÊçÆ
        try:
            mAP_result = self.map_metric.compute()
            mAP = mAP_result['map'].item()
                
        except Exception as e:
            self.logger.warning(f"mAPËÆ°ÁÆóÂ§±Ë¥•: {e}Ôºå‰ΩøÁî®ÈªòËÆ§ÂÄº0.0")
            mAP = 0.0
        
        return avg_val_loss, mAP
    
    def _create_progress_bar(self, epoch: int, mode: str):
        """ÂàõÂª∫ËøõÂ∫¶Êù°"""
        from tqdm import tqdm
        return tqdm(
            self.train_loader if mode == "Training" else self.val_loader,
            desc=f'Epoch {epoch+1}/{self.config["epochs"]} - {mode}'
        )
    
    def _update_progress_bar(self, pbar, loss, loss_items, batch_idx, epoch):
        """Êõ¥Êñ∞ËøõÂ∫¶Êù°"""
        pbar.set_postfix({
            'loss': f'{loss.item():.4f}',
            'box': f'{loss_items[0]:.4f}',
            'cls': f'{loss_items[1]:.4f}',
            'dfl': f'{loss_items[2]:.4f}',
            'lr': f'{self.optimizer.param_groups[0]["lr"]:.6f}',
        })
    
    def _log_training_step(self, loss, loss_items, batch_idx, epoch):
        """ËÆ∞ÂΩïËÆ≠ÁªÉÊ≠•È™§Âà∞TensorBoard"""
        global_step = epoch * len(self.train_loader) + batch_idx
        self.writer.add_scalar('Loss/train_step', loss.item(), global_step)
        self.writer.add_scalar('LR/step', self.optimizer.param_groups[0]['lr'], global_step)
    
    def save_checkpoint(self, epoch: int, is_best: bool = False):
        """‰øùÂ≠òÊ£ÄÊü•ÁÇπÔºàÂåÖÂê´AMPÁä∂ÊÄÅÔºâ"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'config': self.config,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'mAPs': self.mAPs,
            'best_map': self.best_map
        }
        
        # ‰øùÂ≠òAMPÁä∂ÊÄÅ
        if self.use_amp:
            checkpoint['scaler_state_dict'] = self.scaler.state_dict()
        
        # ‰øùÂ≠òÊúÄÊñ∞Ê£ÄÊü•ÁÇπ
        latest_path = f"checkpoint_epoch_{epoch}.pth"
        torch.save(checkpoint, latest_path)
        self.logger.info(f"üíæ ‰øùÂ≠òÊ£ÄÊü•ÁÇπ: {latest_path}")
        
        # ‰øùÂ≠òÊúÄ‰Ω≥Ê®°Âûã
        if is_best:
            best_path = "best_model.pth"
            torch.save(checkpoint, best_path)
            self.logger.info(f"üèÜ ‰øùÂ≠òÊúÄ‰Ω≥Ê®°Âûã: {best_path}")
    
    def load_checkpoint(self, checkpoint_path: str) -> bool:
        """‰ªéÊ£ÄÊü•ÁÇπÊÅ¢Â§çËÆ≠ÁªÉÁä∂ÊÄÅÔºàÂåÖÂê´AMPÁä∂ÊÄÅÔºâ"""
        try:
            self.logger.info(f"üìÇ Âä†ËΩΩÊ£ÄÊü•ÁÇπ: {checkpoint_path}")
            
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            
            # Âä†ËΩΩÊ®°ÂûãÊùÉÈáç
            if 'model_state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['model_state_dict'])
                self.logger.info("‚úì Ê®°ÂûãÊùÉÈáçÂä†ËΩΩÊàêÂäü")
            
            # Âä†ËΩΩ‰ºòÂåñÂô®Áä∂ÊÄÅ
            if 'optimizer_state_dict' in checkpoint:
                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                self.logger.info("‚úì ‰ºòÂåñÂô®Áä∂ÊÄÅÂä†ËΩΩÊàêÂäü")
            
            # Âä†ËΩΩË∞ÉÂ∫¶Âô®Áä∂ÊÄÅ
            if 'scheduler_state_dict' in checkpoint:
                self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                self.logger.info("‚úì Ë∞ÉÂ∫¶Âô®Áä∂ÊÄÅÂä†ËΩΩÊàêÂäü")
            
            # Âä†ËΩΩAMPÁä∂ÊÄÅ
            if self.use_amp and 'scaler_state_dict' in checkpoint:
                self.scaler.load_state_dict(checkpoint['scaler_state_dict'])
                self.logger.info("‚úì AMPÁä∂ÊÄÅÂä†ËΩΩÊàêÂäü")
            
            # Âä†ËΩΩËÆ≠ÁªÉÂéÜÂè≤
            if 'train_losses' in checkpoint:
                self.train_losses = checkpoint['train_losses']
            if 'val_losses' in checkpoint:
                self.val_losses = checkpoint['val_losses']
            if 'mAPs' in checkpoint:
                self.mAPs = checkpoint['mAPs']
            if 'best_map' in checkpoint:
                self.best_map = checkpoint['best_map']
            
            # ËÆæÁΩÆËµ∑Âßãepoch
            if 'epoch' in checkpoint:
                self.start_epoch = checkpoint['epoch'] + 1
                self.logger.info(f"‚úì Â∞Ü‰ªéÁ¨¨ {self.start_epoch} ‰∏™epochÂºÄÂßãËÆ≠ÁªÉ")
            
            # È™åËØÅÈÖçÁΩÆ‰∏ÄËá¥ÊÄß
            if 'config' in checkpoint:
                self._validate_config_consistency(checkpoint['config'])
            
            self.logger.info(f"‚úì Ê£ÄÊü•ÁÇπÊÅ¢Â§çÂÆåÊàêÔºÅÂ∞Ü‰ªéÁ¨¨ {self.start_epoch} ‰∏™epochÁªßÁª≠ËÆ≠ÁªÉ")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Âä†ËΩΩÊ£ÄÊü•ÁÇπÂ§±Ë¥•: {str(e)}")
            return False

    def _update_map_metric(self, preds, targets, signals):
        """Êõ¥Êñ∞mAPÊåáÊ†á - ‰ΩøÁî®‰∏étrain_yolo1d_with_dataset.py‰∏ÄËá¥ÁöÑÈÄªËæë"""
        try:
            # ‰ΩøÁî®postprocess_for_metricsËøõË°åÂêéÂ§ÑÁêÜ
            preds_processed = postprocess_for_metrics(preds, self.model, conf_thresh=0.1)
            
            # Ê†ºÂºèÂåñÁúüÂÆûÊ†áÁ≠æ
            for b in range(signals.shape[0]):
                gt_targets = targets[targets[:, 0] == b]
                if gt_targets.shape[0] > 0:
                    # cx_norm, w_norm -> x1, y1, x2, y2
                    cx_norm = gt_targets[:, 2]
                    w_norm = gt_targets[:, 3]
                    x1 = (cx_norm - w_norm / 2) * self.config['input_length']
                    x2 = (cx_norm + w_norm / 2) * self.config['input_length']
                    
                    boxes = torch.stack([
                        x1,
                        torch.zeros_like(x1),
                        x2,
                        torch.ones_like(x2)
                    ], dim=1)
                    
                    # Êõ¥Êñ∞mAPÊåáÊ†á
                    self.map_metric.update(
                        preds=[preds_processed[b]],
                        target=[{
                            'boxes': boxes,
                            'labels': gt_targets[:, 1].long()
                        }]
                    )
                    
        except Exception as e:
            # Â¶ÇÊûúmAPÊõ¥Êñ∞Â§±Ë¥•ÔºåËÆ∞ÂΩïÈîôËØØ‰ΩÜ‰∏ç‰∏≠Êñ≠ËÆ≠ÁªÉ
            self.logger.debug(f"mAPÊõ¥Êñ∞Â§±Ë¥•: {e}")


class YOLO1DAMPTrainer(AMPTrainer):
    """YOLO1D‰∏ìÁî®AMPËÆ≠ÁªÉÂô®"""
    
    def __init__(self, 
                 train_loader: torch.utils.data.DataLoader,
                 val_loader: torch.utils.data.DataLoader,
                 config: Dict[str, Any],
                 device: torch.device,
                 resume_from: Optional[str] = None):
        """
        ÂàùÂßãÂåñYOLO1D AMPËÆ≠ÁªÉÂô®
        
        Args:
            train_loader: ËÆ≠ÁªÉÊï∞ÊçÆÂä†ËΩΩÂô®
            val_loader: È™åËØÅÊï∞ÊçÆÂä†ËΩΩÂô®
            config: ËÆ≠ÁªÉÈÖçÁΩÆ
            device: ËÆ°ÁÆóËÆæÂ§á
            resume_from: ÊÅ¢Â§çËÆ≠ÁªÉÁöÑÊ£ÄÊü•ÁÇπË∑ØÂæÑ
        """
        from yolo1d_model import create_yolo1d_model
        
        # ÂàõÂª∫Ê®°Âûã
        try:
            model = create_yolo1d_model(
                model_size=config['model_size'],
                num_classes=config['num_classes'],
                input_channels=config['input_channels'],
                input_length=config['input_length']
            )
        except Exception as e:
            raise Exception(f"Ê®°ÂûãÂàùÂßãÂåñÂ§±Ë¥•: {e}")
        
        # Ë∞ÉÁî®Áà∂Á±ªÂàùÂßãÂåñ
        super().__init__(model, train_loader, val_loader, config, device, resume_from)
    
    def _update_map_metric(self, preds, targets, signals):
        """Êõ¥Êñ∞mAPÊåáÊ†á - ‰ΩøÁî®‰∏étrain_yolo1d_with_dataset.py‰∏ÄËá¥ÁöÑÈÄªËæë"""
        try:
            # ‰ΩøÁî®postprocess_for_metricsËøõË°åÂêéÂ§ÑÁêÜ
            preds_processed = postprocess_for_metrics(preds, self.model, conf_thresh=0.1)
            
            # Ê†ºÂºèÂåñÁúüÂÆûÊ†áÁ≠æ
            for b in range(signals.shape[0]):
                gt_targets = targets[targets[:, 0] == b]
                if gt_targets.shape[0] > 0:
                    # cx_norm, w_norm -> x1, y1, x2, y2
                    cx_norm = gt_targets[:, 2]
                    w_norm = gt_targets[:, 3]
                    x1 = (cx_norm - w_norm / 2) * self.config['input_length']
                    x2 = (cx_norm + w_norm / 2) * self.config['input_length']
                    
                    boxes = torch.stack([
                        x1,
                        torch.zeros_like(x1),
                        x2,
                        torch.ones_like(x2)
                    ], dim=1)
                    
                    # Êõ¥Êñ∞mAPÊåáÊ†á
                    self.map_metric.update(
                        preds=[preds_processed[b]],
                        target=[{
                            'boxes': boxes,
                            'labels': gt_targets[:, 1].long()
                        }]
                    )
                    
        except Exception as e:
            # Â¶ÇÊûúmAPÊõ¥Êñ∞Â§±Ë¥•ÔºåËÆ∞ÂΩïÈîôËØØ‰ΩÜ‰∏ç‰∏≠Êñ≠ËÆ≠ÁªÉ
            self.logger.debug(f"mAPÊõ¥Êñ∞Â§±Ë¥•: {e}") 