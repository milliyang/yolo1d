import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import os
import json
from yolo1d_model import create_yolo1d_model
from yolo1d_loss import YOLO1DLoss, compute_loss
from dataset_generator import load_generated_dataset
import yaml
import argparse
from torch.utils.tensorboard import SummaryWriter
from torchmetrics.detection import MeanAveragePrecision
from utils import plot_training_curves, plot_validation_predictions, postprocess_for_metrics
import copy
from pathlib import Path


class SinWaveDataset(Dataset):
    """Sinæ³¢å³°æ£€æµ‹æ•°æ®é›†ç±»"""
    def __init__(self, signals, labels, transform=None):
        """
        Args:
            signals: ä¿¡å·æ•°æ®æ•°ç»„ [N, sequence_length]
            labels: æ ‡ç­¾åˆ—è¡¨ [N, [class_id, x_center, width]]
            transform: æ•°æ®å˜æ¢
        """
        self.signals = signals
        self.labels = labels
        self.transform = transform
    
    def __len__(self):
        return len(self.signals)
    
    def __getitem__(self, idx):
        signal = self.signals[idx]
        
        if self.transform:
            signal = self.transform(signal)
        
        # è½¬æ¢ä¸ºå¼ é‡
        signal = torch.FloatTensor(signal).unsqueeze(0)  # æ·»åŠ é€šé“ç»´åº¦ [1, sequence_length]
        
        # å¤„ç†æ ‡ç­¾ - è½¬æ¢ä¸ºYOLOæ ¼å¼ [image_idx, class, x_center, width]
        sample_labels = self.labels[idx]
        if sample_labels:
            # æ·»åŠ image_idx(åœ¨collate_fnä¸­ä¼šè¢«é‡æ–°è®¾ç½®)
            targets = []
            for label in sample_labels:
                class_id, x_center, width = label
                targets.append([idx, class_id, x_center, width])
            targets = torch.FloatTensor(targets)
        else:
            targets = torch.zeros(0, 4)  # ç©ºæ ‡ç­¾
        
        return signal, targets


def collate_fn(batch):
    """è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•°"""
    signals, targets_list = zip(*batch)
    
    # å †å ä¿¡å·
    signals = torch.stack(signals)
    
    # å¤„ç†æ ‡ç­¾ - æ·»åŠ batchç´¢å¼•
    targets = []
    for i, target in enumerate(targets_list):
        if target.shape[0] > 0:
            # æ›´æ–°batchç´¢å¼•
            target[:, 0] = i
            targets.append(target)
    
    if targets:
        targets = torch.cat(targets, 0)
    else:
        targets = torch.zeros(0, 4)
    
    return signals, targets


class DataAugmentation:
    """æ•°æ®å¢å¼ºç±»"""
    def __init__(self, noise_std=0.05, scale_range=(0.8, 1.2)):
        self.noise_std = noise_std
        self.scale_range = scale_range
    
    def __call__(self, signal):
        # æ·»åŠ å™ªå£°
        if self.noise_std > 0:
            noise = np.random.normal(0, self.noise_std, signal.shape)
            signal = signal + noise
        
        # ç¼©æ”¾
        if self.scale_range:
            scale = np.random.uniform(self.scale_range[0], self.scale_range[1])
            signal = signal * scale
        
        return signal


class Trainer:
    """è®­ç»ƒå™¨ç±»"""
    def __init__(self, model, train_loader, val_loader, device, config, resume_from=None):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        self.config = config
        self.start_epoch = 0  # æ·»åŠ èµ·å§‹epoch
        
        # ä¿®å¤æ­¥é•¿é—®é¢˜
        self.fix_stride_issue()
        
        # å®šä¹‰æŸå¤±è¶…å‚æ•°
        hyp = config.get('hyp', {})
        print(f"Using loss hyperparameters: {hyp}")

        # æŸå¤±å‡½æ•°
        self.criterion = YOLO1DLoss(
            num_classes=config['num_classes'],
            reg_max=config.get('reg_max', 16),
            use_dfl=True,
            hyp=hyp
        )
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=config['learning_rate'],
            weight_decay=config['weight_decay']
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.OneCycleLR(
            self.optimizer,
            max_lr=config['learning_rate'],
            epochs=config['epochs'],
            steps_per_epoch=len(train_loader)
        )
        
        # æ—¥å¿—å’ŒæŒ‡æ ‡
        self.log_dir = Path(f"runs/{config.get('run_name', 'experiment')}")
        self.writer = SummaryWriter(log_dir=str(self.log_dir))
        self.weights_dir = self.log_dir / 'weights'
        self.weights_dir.mkdir(parents=True, exist_ok=True)
        
        self.map_metric = MeanAveragePrecision(
            box_format='xyxy',
            iou_type='bbox'
        )
        
        self.train_losses = []
        self.val_losses = []
        self.mAPs = []
        self.best_map = 0.0
        
        # å¦‚æœæŒ‡å®šäº†æ¢å¤æ£€æŸ¥ç‚¹ï¼Œåˆ™åŠ è½½
        if resume_from:
            self.load_checkpoint(resume_from)
    
    def fix_stride_issue(self):
        """ä¿®å¤æ­¥é•¿é—®é¢˜"""
        print("ğŸ”§ ä¿®å¤æ­¥é•¿é—®é¢˜")
        
        # æ ¹æ®è¾“å…¥é•¿åº¦è®¡ç®—æ­£ç¡®çš„æ­¥é•¿
        input_length = self.config['input_length']
        
        # å…¸å‹çš„YOLOæ­¥é•¿é…ç½®
        if input_length == 1024:
            correct_strides = [8, 16, 32]  # å¸¸è§çš„æ­¥é•¿é…ç½®
        else:
            # æ ¹æ®è¾“å…¥é•¿åº¦åŠ¨æ€è®¡ç®—
            correct_strides = [input_length // 128, input_length // 64, input_length // 32]
        
        print(f"  è¾“å…¥é•¿åº¦: {input_length}")
        print(f"  è®¡ç®—å‡ºçš„æ­£ç¡®æ­¥é•¿: {correct_strides}")
        
        # ä¿®å¤æ­¥é•¿
        with torch.no_grad():
            for i, stride in enumerate(correct_strides):
                self.model.detect.stride[i] = float(stride)
        
        print(f"  ä¿®å¤åçš„æ­¥é•¿: {self.model.detect.stride}")
    
    def load_checkpoint(self, checkpoint_path):
        """ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒçŠ¶æ€ï¼Œæ”¯æŒç»å¯¹è·¯å¾„å’Œç›¸å¯¹è·¯å¾„ã€‚"""
        resume_path = Path(checkpoint_path)
        if not resume_path.is_file():
            print(f"âŒ é”™è¯¯: æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸æ˜¯ä¸€ä¸ªæ–‡ä»¶ -> {resume_path}")
            return False
            
        try:
            print(f"ğŸ“‚ æ­£åœ¨ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {resume_path.resolve()}")
            checkpoint = torch.load(resume_path, map_location=self.device)
            
            # åŠ è½½æ¨¡å‹çŠ¶æ€
            if 'model_state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['model_state_dict'])
                print("âœ“ æ¨¡å‹çŠ¶æ€å·²åŠ è½½")
            
            # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
            if 'optimizer_state_dict' in checkpoint:
                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                print("âœ“ ä¼˜åŒ–å™¨çŠ¶æ€å·²åŠ è½½")
            
            # åŠ è½½å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€
            if 'scheduler_state_dict' in checkpoint:
                self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                print("âœ“ å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€å·²åŠ è½½")
            
            # åŠ è½½è®­ç»ƒå†å²
            if 'train_losses' in checkpoint:
                self.train_losses = checkpoint['train_losses']
                print(f"âœ“ è®­ç»ƒæŸå¤±å†å²å·²åŠ è½½ (å…±{len(self.train_losses)}ä¸ªepoch)")
            
            if 'val_losses' in checkpoint:
                self.val_losses = checkpoint['val_losses']
                print(f"âœ“ éªŒè¯æŸå¤±å†å²å·²åŠ è½½ (å…±{len(self.val_losses)}ä¸ªepoch)")
            
            if 'mAPs' in checkpoint:
                self.mAPs = checkpoint['mAPs']
                print(f"âœ“ mAPå†å²å·²åŠ è½½ (å…±{len(self.mAPs)}ä¸ªepoch)")
            
            # åŠ è½½æœ€ä½³mAP
            if 'best_map' in checkpoint:
                self.best_map = checkpoint['best_map']
                print(f"âœ“ æœ€ä½³mAPå·²åŠ è½½: {self.best_map:.4f}")
            
            # è®¾ç½®èµ·å§‹epoch
            if 'epoch' in checkpoint:
                self.start_epoch = checkpoint['epoch'] + 1
                print(f"âœ“ å°†ä»ç¬¬ {self.start_epoch} ä¸ªepochå¼€å§‹è®­ç»ƒ")
            
            # éªŒè¯é…ç½®ä¸€è‡´æ€§
            if 'config' in checkpoint:
                saved_config = checkpoint['config']
                current_config = self.config
                
                # æ£€æŸ¥å…³é”®é…ç½®æ˜¯å¦ä¸€è‡´
                key_configs = ['model_size', 'num_classes', 'input_channels', 'input_length']
                config_mismatch = []
                for key in key_configs:
                    if key in saved_config and key in current_config:
                        if saved_config[key] != current_config[key]:
                            config_mismatch.append(f"{key}: {saved_config[key]} -> {current_config[key]}")
                
                if config_mismatch:
                    print("âš ï¸  é…ç½®ä¸åŒ¹é…:")
                    for mismatch in config_mismatch:
                        print(f"    {mismatch}")
                    print("å»ºè®®ä½¿ç”¨ä¸æ£€æŸ¥ç‚¹ç›¸åŒçš„é…ç½®è¿›è¡Œæ¢å¤è®­ç»ƒ")
                else:
                    print("âœ“ é…ç½®ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡")
            
            print(f"âœ“ æ£€æŸ¥ç‚¹æ¢å¤å®Œæˆï¼å°†ä»ç¬¬ {self.start_epoch} ä¸ªepochç»§ç»­è®­ç»ƒ")
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {str(e)}")
            return False
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        epoch_loss = 0.0
        
        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}/{self.config["epochs"]} - Training')
        for batch_idx, (signals, targets) in enumerate(pbar):
            signals = signals.to(self.device)
            targets = targets.to(self.device)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            preds = self.model(signals)
            
            # è®¡ç®—æŸå¤±
            loss, loss_items = compute_loss(preds, targets, self.model, self.criterion)
            
            # æ£€æŸ¥æŸå¤±æ˜¯å¦æœ‰æ•ˆ
            if torch.isnan(loss) or torch.isinf(loss):
                print(f"Warning: Invalid loss at batch {batch_idx}: {loss}")
                continue
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=10.0)
            
            self.optimizer.step()
            self.scheduler.step()
            
            # è®°å½•æŸå¤±
            epoch_loss += loss.item()
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'box': f'{loss_items[0]:.4f}',
                'cls': f'{loss_items[1]:.4f}',
                'dfl': f'{loss_items[2]:.4f}',
            })
            
            # è®°å½•åˆ°TensorBoard (æ¯ä¸ªstep)
            global_step = epoch * len(self.train_loader) + batch_idx
            self.writer.add_scalar('Loss/train_step', loss.item(), global_step)
            self.writer.add_scalar('LR/step', self.optimizer.param_groups[0]['lr'], global_step)

        return epoch_loss / len(self.train_loader)
    
    def validate(self, epoch):
        """éªŒè¯, è®¡ç®—æŸå¤±å’ŒmAP"""
        self.model.eval()
        val_loss = 0.0
        
        preds_for_map = []
        targets_for_map = []
        
        # å­˜å‚¨ç¬¬ä¸€ä¸ªbatchç”¨äºå¯è§†åŒ–
        first_batch_data_for_vis = None

        with torch.no_grad():
            pbar = tqdm(self.val_loader, desc=f'Epoch {epoch+1}/{self.config["epochs"]} - Validation')
            for signals, targets in pbar:
                signals = signals.to(self.device)
                targets = targets.to(self.device)
                
                # æ¨ç†
                preds_raw = self.model(signals)
                
                # è®¡ç®—æŸå¤±
                loss, _ = compute_loss(preds_raw, targets, self.model, self.criterion)
                if not (torch.isnan(loss) or torch.isinf(loss)):
                    val_loss += loss.item()
                
                # --- ä¸ºmAPæŒ‡æ ‡æ ¼å¼åŒ–è¾“å‡º ---
                preds_processed = postprocess_for_metrics(preds_raw, self.model, conf_thresh=0.1)  # é™ä½ç½®ä¿¡åº¦é˜ˆå€¼
                preds_for_map.extend(preds_processed)
                
                # æ ¼å¼åŒ–çœŸå®æ ‡ç­¾
                for b in range(signals.shape[0]):
                    gt_targets = targets[targets[:, 0] == b]
                    # cx_norm, w_norm -> x1, y1, x2, y2
                    cx_norm = gt_targets[:, 2]
                    w_norm = gt_targets[:, 3]
                    x1 = (cx_norm - w_norm / 2) * self.config['input_length']
                    x2 = (cx_norm + w_norm / 2) * self.config['input_length']
                    
                    boxes = torch.stack([
                        x1,
                        torch.zeros_like(x1),
                        x2,
                        torch.ones_like(x2)
                    ], dim=1)
                    targets_for_map.append({'boxes': boxes, 'labels': gt_targets[:, 1].long()})

                # ä¿å­˜ç¬¬ä¸€ä¸ªbatchç”¨äºå¯è§†åŒ–
                if first_batch_data_for_vis is None:
                    first_batch_data_for_vis = {
                        "signals": signals.cpu(),
                        "preds": copy.deepcopy(preds_processed),
                        "gts": copy.deepcopy(targets_for_map[:signals.shape[0]])
                    }

        # --- è®¡ç®—æŒ‡æ ‡ ---
        self.map_metric.update(preds_for_map, targets_for_map)
        map_stats = self.map_metric.compute()
        self.map_metric.reset()
        
        final_val_loss = val_loss / len(self.val_loader)

        # --- æ—¥å¿—è®°å½• ---
        self.writer.add_scalar('Loss/validation', final_val_loss, epoch)
        self.writer.add_scalar('mAP/0.5', map_stats['map_50'], epoch)
        self.writer.add_scalar('mAP/0.5:0.95', map_stats['map'], epoch)

        return final_val_loss, map_stats['map_50'].item()
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"å¼€å§‹è®­ç»ƒï¼Œè®¾å¤‡: {self.device}")
        print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"æ—¥å¿—ä¿å­˜åœ¨: {self.writer.log_dir}")
        print(f"è®­ç»ƒå°†ä»ç¬¬ {self.start_epoch + 1} ä¸ªepochå¼€å§‹ï¼Œæ€»å…± {self.config['epochs']} ä¸ªepoch")
        
        for epoch in range(self.start_epoch, self.config['epochs']):
            # è®­ç»ƒ
            train_loss = self.train_epoch(epoch)
            self.train_losses.append(train_loss)
            self.writer.add_scalar('Loss/train_epoch', train_loss, epoch)
            
            # éªŒè¯
            val_loss, current_map = self.validate(epoch)
            self.val_losses.append(val_loss)
            self.mAPs.append(current_map)
            
            print(f"Epoch {epoch+1}/{self.config['epochs']} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, mAP@0.5: {current_map:.4f}")
            
            # å¯è§†åŒ–ä¸€ä¸ªéªŒè¯æ ·æœ¬åˆ°TensorBoard (æ¯ä¸ªepoch)
            self.visualize_to_tensorboard(epoch)

            # ä¿å­˜æœ€ä½³æ¨¡å‹ (åŸºäºmAP)
            if current_map > self.best_map:
                self.best_map = current_map
                self.save_checkpoint(epoch, 'best_model.pth')
                print(f"æ–°çºªå½•! ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒmAP@0.5: {current_map:.4f}")
            
            # å®šæœŸä¿å­˜
            if (epoch + 1) % 10 == 0:
                self.save_checkpoint(epoch, f'checkpoint_epoch_{epoch+1}.pth')
        
        # ç»˜åˆ¶æœ€ç»ˆè®­ç»ƒæ›²çº¿
        plot_training_curves(self.train_losses, self.val_losses, self.mAPs)
        self.writer.close()
        print("è®­ç»ƒå®Œæˆï¼")
    
    def visualize_to_tensorboard(self, epoch):
        """å°†ä¸€ä¸ªbatchçš„é¢„æµ‹ç»“æœå¯è§†åŒ–åˆ°TensorBoard"""
        self.model.eval()
        # ä½¿ç”¨ä¸€ä¸ªå›ºå®šçš„éªŒè¯batchè¿›è¡Œå¯è§†åŒ–
        signals, targets = next(iter(self.val_loader))
        signals = signals.to(self.device)

        with torch.no_grad():
            preds_raw = self.model(signals)
            preds_processed = postprocess_for_metrics(preds_raw, self.model)

        # æ ¼å¼åŒ–GT
        gt_targets = targets[targets[:, 0] == 0] # åªå–ç¬¬ä¸€ä¸ªæ ·æœ¬
        # cx_norm, w_norm -> x1, y1, x2, y2
        cx_norm = gt_targets[:, 2]
        w_norm = gt_targets[:, 3]
        x1 = (cx_norm - w_norm / 2) * self.config['input_length']
        x2 = (cx_norm + w_norm / 2) * self.config['input_length']
        gt_boxes = torch.stack([x1, torch.zeros_like(x1), x2, torch.ones_like(x2)], dim=1)

        gt_for_vis = {'boxes': gt_boxes, 'labels': gt_targets[:, 1].long()}
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ ·æœ¬è¿›è¡Œç»˜å›¾
        fig = plot_validation_predictions(
            signals[0].cpu().numpy().flatten(),
            gt_for_vis,
            preds_processed[0],
            self.config['class_names'],
            title=f"Validation Prediction at Epoch {epoch+1}"
        )
        self.writer.add_figure('Validation/prediction_sample', fig, global_step=epoch)

    def save_checkpoint(self, epoch, filename):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'mAPs': self.mAPs,
            'best_map': self.best_map,
            'config': self.config
        }
        save_path = self.weights_dir / filename
        torch.save(checkpoint, save_path)
        print(f"ğŸ’¾ Checkpoint saved to {save_path}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Train YOLOv1D model on a custom dataset.")
    parser.add_argument('--config', type=str, default='config.yaml', help='Path to the YAML configuration file.')
    parser.add_argument('--resume', type=str, default=None, help='Path to checkpoint file to resume training from.')
    args = parser.parse_args()

    # åŠ è½½é…ç½®
    try:
        with open(args.config, 'r') as f:
            config = yaml.safe_load(f)
        print("YAML configuration loaded successfully.")
    except Exception as e:
        print(f"Error loading YAML file: {e}")
        return
    
    # è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®é›†
    if not os.path.exists(config['dataset_path']):
        print(f"æ•°æ®é›†ä¸å­˜åœ¨: {config['dataset_path']}")
        print("è¯·å…ˆè¿è¡Œ dataset_generator.py ç”Ÿæˆæ•°æ®é›†")
        return
    
    try:
        (train_signals, train_labels), (val_signals, val_labels), dataset_info = load_generated_dataset(
            config['dataset_path']
        )
        print(f"æ•°æ®é›†åŠ è½½æˆåŠŸ:")
        print(f"  è®­ç»ƒæ ·æœ¬: {len(train_signals)}")
        print(f"  éªŒè¯æ ·æœ¬: {len(val_signals)}")
        print(f"  ç±»åˆ«æ•°: {dataset_info['num_classes']}")
        print(f"  ç±»åˆ«åç§°: {dataset_info['class_names']}")
        
        # æ›´æ–°é…ç½®
        config['num_classes'] = dataset_info['num_classes']
        config['input_length'] = dataset_info['sequence_length']
        config['class_names'] = dataset_info['class_names']
        # æ·»åŠ ä¸€ä¸ªè¿è¡Œåç§°
        config['run_name'] = f"run_{config['model_size']}_bs{config['batch_size']}_{config['epochs']}e"
        
    except Exception as e:
        print(f"åŠ è½½æ•°æ®é›†å¤±è´¥: {str(e)}")
        return
    
    # åˆ›å»ºæ¨¡å‹
    print("ğŸ—ï¸  åˆ›å»ºæ¨¡å‹...")
    model = create_yolo1d_model(
        model_size=config['model_size'],
        num_classes=config['num_classes'],
        input_channels=config['input_channels'],
        input_length=config['input_length'],
        reg_max=config.get('reg_max', 16)
    )
    
    # æ•°æ®å¢å¼º
    train_transform = DataAugmentation(noise_std=0.02, scale_range=(0.9, 1.1))
    
    # æ•°æ®é›†
    train_dataset = SinWaveDataset(train_signals, train_labels, transform=train_transform)
    val_dataset = SinWaveDataset(val_signals, val_labels, transform=None)
    
    # æ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['batch_size'],
        shuffle=True,
        num_workers=config['num_workers'],
        collate_fn=collate_fn,
        pin_memory=True if device.type == 'cuda' else False
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        num_workers=config['num_workers'],
        collate_fn=collate_fn,
        pin_memory=True if device.type == 'cuda' else False
    )
    
    # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡ä¿¡æ¯
    print(f"\næ•°æ®ç»Ÿè®¡:")
    print(f"  æ‰¹å¤§å°: {config['batch_size']}")
    print(f"  è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}")
    print(f"  éªŒè¯æ‰¹æ¬¡: {len(val_loader)}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    print("ğŸš‚ åˆ›å»ºè®­ç»ƒå™¨...")
    trainer = Trainer(model, train_loader, val_loader, device, config, resume_from=args.resume)
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()
    
    print("\nè®­ç»ƒå®Œæˆï¼")
    print("å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œæ¨ç†:")
    print("python inference_yolo1d.py --model best_model.pth --signal-type anomaly")


if __name__ == "__main__":
    main() 